# Core dependencies
fastapi>=0.100.0
uvicorn>=0.23.0

# llama.cpp backend (install with Metal support: CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python)
llama-cpp-python>=0.3.0

# Optional: Keep transformers for future cloud API integrations
# torch>=2.0.0
# transformers>=4.30.0
# accelerate>=0.20.0
# sentencepiece>=0.1.99

